{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "from datasets import Dataset,load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'T5ForConditionalGeneration' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "c:\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'How to cook something?omething? cook something? cooking cooking cooking? cooking cooking? cooking cooking?'}]\n"
     ]
    }
   ],
   "source": [
    "model_pt = 'google-t5/t5-small'\n",
    "# model_pt = 'FacebookAI/roberta-base'\n",
    "# model_pt = 'google/flan-t5-xxl'\n",
    "# model_pt = 'google-t5/t5-large'\n",
    "tg = pipeline(task='text-generation',model=model_pt)\n",
    "\n",
    "print(tg('How to cook something?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Input', 'Output'],\n",
       "        num_rows: 80\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Input', 'Output'],\n",
       "        num_rows: 20\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./datasets/train_data.csv')\n",
    "df = df.iloc[:100,:]\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 80/80 [00:00<00:00, 2376.46 examples/s]\n",
      "Map: 100%|██████████| 20/20 [00:00<00:00, 1828.10 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_pt)\n",
    "def tokenize_function(examples):\n",
    "    inputs = tokenizer(examples['Input'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    outputs = tokenizer(examples['Output'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    inputs['labels'] = outputs['input_ids']\n",
    "    return inputs\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_pt)\n",
    "\n",
    "# Remove columns that the model doesn't expect and set the format for PyTorch\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"Input\", \"Output\"])\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "train_dataloader = DataLoader(tokenized_datasets[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator)\n",
    "eval_dataloader = DataLoader(tokenized_datasets[\"test\"], batch_size=8, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvishwateja2684\u001b[0m (\u001b[33mvishwa-teja\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\vishw\\Desktop\\chefbot\\model\\wandb\\run-20240530_162811-it36o9mh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vishwa-teja/huggingface/runs/it36o9mh' target=\"_blank\">balmy-energy-17</a></strong> to <a href='https://wandb.ai/vishwa-teja/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vishwa-teja/huggingface' target=\"_blank\">https://wandb.ai/vishwa-teja/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vishwa-teja/huggingface/runs/it36o9mh' target=\"_blank\">https://wandb.ai/vishwa-teja/huggingface/runs/it36o9mh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [07:05<00:00, 10.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 429.0224, 'train_samples_per_second': 0.932, 'train_steps_per_second': 0.093, 'train_loss': 10.405975341796875, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=40, training_loss=10.405975341796875, metrics={'train_runtime': 429.0224, 'train_samples_per_second': 0.932, 'train_steps_per_second': 0.093, 'total_flos': 54136720588800.0, 'train_loss': 10.405975341796875, 'epoch': 5.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, T5ForConditionalGeneration\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_pt)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=10,\n",
    "    per_device_eval_batch_size=10,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Kesar Mango Lassi Recipe - Saffron Mango Lassi Recipe\n",
      "Generated Output: Kesar Mango Lassi Recipe - Saffron Mango Lassi Recipe - Kesar Mango Lassi Recipe - Saffron Mango Lassi Recipe - Kesar Mango Lassi Recipe - Kesar Mango Lassi Recipe - Saffron Mango Lassi Recipe - Kesar Mango Lassi Recipe - Kesar Mango Lassi Recipe - Saffron Mango Lassi Recipe\n",
      "Reference Output: 3/4 cup Mango Pulp (Puree),1 cup Curd (Dahi / Yogurt) - (low fat),1 tablespoon Sugar - or honey,2 pinch Saffron strands,3 tablespoons Badam (Almond) - roughly chopped,3 tablespoons Milk - luke warm+more cold milk as needed,Ice cubes - a few To begin making Kesar Mango Lassi Recipe, soak one pinch kesar in 2 tablespoons warm milk and keep it aside for few minutes.Get prep with other ingredients as well. Take out the pulp from the mangoes and keep aside.In a blender, add mango pulp, curd/yogurt, cold milk and blend till combined.Once it is done, add chopped almonds (2 tablespoons), honey or sugar, some ice cubes and blend to make the Kesar Mango Lassi Recipe.Kesar Mango Lassi Recipe is ready to serve, garnish it with the rest of chopped almonds and saffron and serve with Bhel Puri Recipe or Asian Watermelon Salad Recipe during evening.\n",
      "--------------------------------------------------\n",
      "Input: Basil Tincture Recipe (Or) - Herbal Drink From Basil\n",
      "Generated Output: Basil Tincture Recipe recipe (Or) - Herbal Drink From Basil, The Basil Tincture Recipes of Basil Tincture Recipe (Or) - herbal drink from Basil in Shebama and Basil Injuries: basil Tincture Recipe for Basil Tincture Recipe For Basil Tincture Recipe(Or) Ayurved to Basil Lemon Mixe with Basil Numitate!...It’s the best way to treat Basil.\n",
      "Reference Output: 2 cups Water,1 Tulsi (holy basil),1/2 teaspoon Black pepper powder,1/2 teaspoon Dry ginger powder,1 teaspoon Palm sugar - (Panakalkandu) To begin making the Kashayam Recipe, prep all the ingredients and keep them handy.Add water into a sauce pan, tear the tulsi leaves and give it a brisk boil.Once the color of the water changes, add the black pepper powder, ginger powder, palm sugar (Panakalkandu), boil for a few more minutes and turn off the heat.Serve the Tulsi Kashayam hot and have it 2 to 3 times a day to get maximum relief from cough and cold.\n",
      "--------------------------------------------------\n",
      "Input: Masala Karela Recipe\n",
      "Generated Output: Masala Karela Recipe - recipe for Masala Karela Recipes! Die recette für Masala Karela Recipe in der M Masala Karela Recipe Cook the Masala Karela Recipe and recipes to follow Masala Karela Recipe Ingredient Mix Sha Masala Karela Recipe (Mo Masala Karela Recipe) Inm Masala Karela Recipe I Rind Mas Masala Karela Recipe\n",
      "Reference Output: 6 Karela (Bitter Gourd/ Pavakkai) - deseeded,Salt - to taste,1 Onion - thinly sliced,3 tablespoon Gram flour (besan),2 teaspoons Turmeric powder (Haldi),1 tablespoon Red Chilli powder,2 teaspoons Cumin seeds (Jeera),1 tablespoon Coriander Powder (Dhania),1 tablespoon Amchur (Dry Mango Powder),Sunflower Oil - as required To begin making the Masala Karela Recipe,de-seed the karela and slice. Do not remove the skin as the skin has all the nutrients. Add the karela to the pressure cooker with 3 tablespoon of water, salt and turmeric powder and pressure cook for three whistles. Release the pressure immediately and open the lids. Keep aside.Heat oil in a heavy bottomed pan or a kadhai. Add cumin seeds and let it sizzle.Once the cumin seeds have sizzled, add onions and saute them till it turns golden brown in color.Add the karela, red chilli powder, amchur powder, coriander powder and besan. Stir to combine the masalas into the karela.Drizzle a little extra oil on the top and mix again. Cover the pan and simmer Masala Karela stirring occasionally until everything comes together well. Turn off the heat.Transfer Masala Karela into a serving bowl and serve.Serve Masala Karela along with Panchmel Dal and Phulka for a weekday meal with your family.\n",
      "--------------------------------------------------\n",
      "Input: White Pumpkin Mor Kuzhambu Recipe - Kerala Style White Pumpkin Curry\n",
      "Generated Output: White Pumpkin Mor Kuzhambu Recipe - Kerala Style White Pumpkin Curry in Indian Indians (White Pumpkin Mor Kuzhambu Recipe) white pumpkin mor Kuzhambu Recipe - Kerala Style White Pumpkin Curry im Indian Indian Indian Indian Indian Indian Indian Indian Indian Indian Indian Indian Indian Indian Indian Indian Indian Indian Indian Indian Indian Indian Indian Indian Indian Indian Indian Indian Indian Indian Indian Indian Indian Indian Indian Indian Indian Indian Indian Indian Indian Indian Indian Indian Indian Indian Indian Indian Indian Indian Indian Indian Indian Indian Indian Indian Indian Indian\n",
      "Reference Output: 1-1/2 cups Curd (Dahi / Yogurt) - thick,1/2 cup Fresh coconut,1 cup Vellai Poosanikai (Ash gourd/White Pumpkin) - cubed,4 Green Chillies,1/4 teaspoon Black pepper powder,1/3 cup Water,1/4 teaspoon Turmeric powder (Haldi),Salt - as per your taste,1 teaspoon Coconut Oil,1/4 teaspoon Mustard seeds,1/4 teaspoon Methi Seeds (Fenugreek Seeds),1 sprig Curry leaves,1 Dry Red Chilli To begin making White Pumpkin Mor Kuzhambu, we need to first cook the pumpkin. Pressure cook the white pumpkin with little water in a pressure cooker for 2 whistles and release the pressure immediately by running the cooker under cold water. Keep aside.Meanwhile, whisk curd with water in a bowl until the its smooth. Make sure that there are no lumps in the curd.Next, add grated coconut, green chilli, black pepper powder in a mixer grinder and grind it to a smooth paste with a little water. Your Kuzhambu paste is ready.Transfer this kuzhambu paste to the curd and mix well. Once it is mixed properly, add in turmeric powder, cooked white pumpkin, salt and mix everything well. Place this mixture on low heat in a saucepan and allow the mixture to become warm. You will notice a light froth around the edges of the vessel. Do not let the mixture boil, the curd will split on high heat. Turn off the flame the moment you see froth. The next step is to temper the kuzhambu.In a tempering pan/tadka pan, add one teaspoon of coconut oil. Once the oil is hot, add mustard seeds and fenugreek seeds, Once the mustard seeds starts to crackle, add the curry leaves, dry red chilli and give it a mix. Turn off the flame and add this tempering to the Mor Kuzhambu. Your Mor Kuzhambu is ready to be served. Serve Mor Kuzhambu with Steamed Rice and Menthia Keerai Paruppu Usili for a simple weekday lunch.\n",
      "--------------------------------------------------\n",
      "Input: Spicy Chilli Garlic Noodles Recipe\n",
      "Generated Output: Recipe of Spicy Chilli Garlic Noodles Recipe for picy Chilli Garlic Nuggets recipe in the kitchen with fresh herbs and spices, as well as spicy spiced garlic cloves to add flavor to your dishes. Then you'll be amazed at how delicious it can be enjoyed by those who have been enjoying their own life together on our home page or just want to know more about these wonderful recipes! Here are my favorite recipes from this blog post\n",
      "Reference Output: 500 grams Hakka Noodles,Water - as required,1/2 teaspoon Salt,1 tablespoon Sunflower Oil,Sunflower Oil - as required,1 Onion - thinly sliced,1 Green Bell Pepper (Capsicum) - thinly sliced,15 cloves Garlic - crushed,1 teaspoon Soy sauce,1 tablespoon Red Chilli sauce,1/2 teaspoon White vinegar,1 tablespoon Tomato Ketchup,3 Dry Red Chillies - crushed (chilli flakes),Salt - as required,1/2 teaspoon Black pepper powder To begin making the Spicy Chilli Garlic Noodles recipe, first cook the noodles as per the instructions on the packet, with enough water, salt and oil.Put a pot of water with oil and salt, on the heat and bring it to a rolling boil.When it boils, turn the heat down, add the noodles to it and cook for 5-6 minutes or as specified on the packet, till it is cooked al dente.Take care not to over cook the noodles or they may turn mushy. When the noodles are done, drain the noodles in a colander and rinse under cold water to stop the cooking process.Sprinkle a few drops of oil over the noodles and toss well till they are well coated. This will keep the noodles from getting mushy and stuck to each other. Now you can keep them aside for later use.To begin making the spicy seasoning, place a heavy bottomed pan on the heat. Add some oil and warm it up. Then add the finely chopped garlic and thin slices of onions and toss on a high heat for a minute.Add the capsicum and cook it for about a minute or two.Add soya sauce, red chilli sauce, vinegar, tomato ketchup, crushed dry red chilli and mix well. Cook with constant stirring for about 30 seconds till the sauces begin to bubble.Add salt to taste, pepper powder and mix well. Bear in mind that the sauce and noodles will already contain saltiness so season accordingly.Next, add the cooked noodles to the prepared sauce and toss well to cot the noodles.Serve Spicy Chilli Garlic Noodles on its own or with Mushroom Chilli for a weekend meal.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# model.eval()\n",
    "\n",
    "# # Function to generate text from input\n",
    "# def generate_text(input_text):\n",
    "#     # Tokenize the input text\n",
    "#     input_ids = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).input_ids\n",
    "#     # Generate the output\n",
    "#     with torch.no_grad():\n",
    "#         generated_ids = model.generate(input_ids, max_length=512)\n",
    "#     # Decode the generated tokens to text\n",
    "#     generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "#     return generated_text\n",
    "\n",
    "# # Load a few examples from the test dataset\n",
    "# test_examples = tokenized_datasets[\"test\"].shuffle().select(range(5))\n",
    "\n",
    "# # Generate outputs for these examples\n",
    "# for example in test_examples:\n",
    "#     input_text = tokenizer.decode(example['input_ids'], skip_special_tokens=True)\n",
    "#     generated_text = generate_text(input_text)\n",
    "#     reference_output = tokenizer.decode(example['labels'], skip_special_tokens=True)\n",
    "    \n",
    "#     print(f\"Input: {input_text}\")\n",
    "#     print(f\"Generated Output: {generated_text}\")\n",
    "#     print(f\"Reference Output: {reference_output}\")\n",
    "#     print(\"-\" * 50)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Function to generate text from input\n",
    "def generate_text(input_text, max_length=512, min_length=100, num_beams=5, repetition_penalty=2.5):\n",
    "    # Tokenize the input text\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024).input_ids\n",
    "    # Generate the output\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            input_ids, \n",
    "            max_length=max_length, \n",
    "            min_length=min_length, \n",
    "            num_beams=num_beams, \n",
    "            repetition_penalty=repetition_penalty,\n",
    "            early_stopping=True  # To stop when an EOS token is generated\n",
    "        )\n",
    "    # Decode the generated tokens to text\n",
    "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# Test the model on some examples\n",
    "test_examples = tokenized_datasets[\"test\"].shuffle().select(range(5))\n",
    "\n",
    "for example in test_examples:\n",
    "    input_text = tokenizer.decode(example['input_ids'], skip_special_tokens=True)\n",
    "    generated_text = generate_text(input_text,repetition_penalty=5.0)\n",
    "    reference_output = tokenizer.decode(example['labels'], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"Input: {input_text}\")\n",
    "    print(f\"Generated Output: {generated_text}\")\n",
    "    print(f\"Reference Output: {reference_output}\")\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
