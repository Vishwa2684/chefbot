{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import accelerate\n",
    "from dataclasses import fields,dataclass\n",
    "import pandas as pd\n",
    "from typing import Optional\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig,prepare_model_for_kbit_training\n",
    "from transformers import (AutoTokenizer, \n",
    "                        AutoModelForCausalLM,\n",
    "                        pipeline,\n",
    "                        BitsAndBytesConfig,\n",
    "                        GemmaTokenizer,\n",
    "                        HfArgumentParser,\n",
    "                        TrainingArguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import interpreter_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
      "    Setting a new token will erase the existing one.\n",
      "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
      "Token can be pasted using 'Right-Click'.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\vishw\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "interpreter_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./datasets/modify2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_rows(row):\n",
    "    q=row['Input']\n",
    "    a=row['Output']\n",
    "    # Formatting our dataset to LLaMa format\n",
    "    format = f'[INST] {q} [/INST] {a}'\n",
    "    return format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input</th>\n",
       "      <th>Output</th>\n",
       "      <th>formatted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Masala Karela Recipe</td>\n",
       "      <td>6 Karela (Bitter Gourd/ Pavakkai) - deseeded,S...</td>\n",
       "      <td>[INST] Masala Karela Recipe [/INST] 6 Karela (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ragi Semiya Upma Recipe - Ragi Millet Vermicel...</td>\n",
       "      <td>1-1/2 cups Rice Vermicelli Noodles (Thin),1 On...</td>\n",
       "      <td>[INST] Ragi Semiya Upma Recipe - Ragi Millet V...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gongura Chicken Curry Recipe - Andhra Style Go...</td>\n",
       "      <td>500 grams Chicken,2 Onion - chopped,1 Tomato -...</td>\n",
       "      <td>[INST] Gongura Chicken Curry Recipe - Andhra S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pudina Khara Pongal Recipe (Rice and Lentils C...</td>\n",
       "      <td>1 cup Rice - soaked for 20 minutes,1/2 cup Yel...</td>\n",
       "      <td>[INST] Pudina Khara Pongal Recipe (Rice and Le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Udupi Style Ash Gourd Coconut Curry Recipe</td>\n",
       "      <td>500 grams Vellai Poosanikai (Ash gourd/White P...</td>\n",
       "      <td>[INST] Udupi Style Ash Gourd Coconut Curry Rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250781</th>\n",
       "      <td>zydeco soup</td>\n",
       "      <td>['celery', 'onion', 'green sweet pepper', 'gar...</td>\n",
       "      <td>[INST] zydeco soup [/INST] ['celery', 'onion',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250782</th>\n",
       "      <td>zydeco spice mix</td>\n",
       "      <td>['paprika', 'salt', 'garlic powder', 'onion po...</td>\n",
       "      <td>[INST] zydeco spice mix [/INST] ['paprika', 's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250783</th>\n",
       "      <td>zydeco ya ya deviled eggs</td>\n",
       "      <td>['hard-cooked eggs', 'mayonnaise', 'dijon must...</td>\n",
       "      <td>[INST] zydeco ya ya deviled eggs [/INST] ['har...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250784</th>\n",
       "      <td>cookies by design   cookies on a stick</td>\n",
       "      <td>['butter', 'eagle brand condensed milk', 'ligh...</td>\n",
       "      <td>[INST] cookies by design   cookies on a stick ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250785</th>\n",
       "      <td>cookies by design   sugar shortbread cookies</td>\n",
       "      <td>['granulated sugar', 'shortening', 'eggs', 'fl...</td>\n",
       "      <td>[INST] cookies by design   sugar shortbread co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250786 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Input  \\\n",
       "0                                    Masala Karela Recipe   \n",
       "1       Ragi Semiya Upma Recipe - Ragi Millet Vermicel...   \n",
       "2       Gongura Chicken Curry Recipe - Andhra Style Go...   \n",
       "3       Pudina Khara Pongal Recipe (Rice and Lentils C...   \n",
       "4              Udupi Style Ash Gourd Coconut Curry Recipe   \n",
       "...                                                   ...   \n",
       "250781                                        zydeco soup   \n",
       "250782                                   zydeco spice mix   \n",
       "250783                          zydeco ya ya deviled eggs   \n",
       "250784             cookies by design   cookies on a stick   \n",
       "250785       cookies by design   sugar shortbread cookies   \n",
       "\n",
       "                                                   Output  \\\n",
       "0       6 Karela (Bitter Gourd/ Pavakkai) - deseeded,S...   \n",
       "1       1-1/2 cups Rice Vermicelli Noodles (Thin),1 On...   \n",
       "2       500 grams Chicken,2 Onion - chopped,1 Tomato -...   \n",
       "3       1 cup Rice - soaked for 20 minutes,1/2 cup Yel...   \n",
       "4       500 grams Vellai Poosanikai (Ash gourd/White P...   \n",
       "...                                                   ...   \n",
       "250781  ['celery', 'onion', 'green sweet pepper', 'gar...   \n",
       "250782  ['paprika', 'salt', 'garlic powder', 'onion po...   \n",
       "250783  ['hard-cooked eggs', 'mayonnaise', 'dijon must...   \n",
       "250784  ['butter', 'eagle brand condensed milk', 'ligh...   \n",
       "250785  ['granulated sugar', 'shortening', 'eggs', 'fl...   \n",
       "\n",
       "                                                formatted  \n",
       "0       [INST] Masala Karela Recipe [/INST] 6 Karela (...  \n",
       "1       [INST] Ragi Semiya Upma Recipe - Ragi Millet V...  \n",
       "2       [INST] Gongura Chicken Curry Recipe - Andhra S...  \n",
       "3       [INST] Pudina Khara Pongal Recipe (Rice and Le...  \n",
       "4       [INST] Udupi Style Ash Gourd Coconut Curry Rec...  \n",
       "...                                                   ...  \n",
       "250781  [INST] zydeco soup [/INST] ['celery', 'onion',...  \n",
       "250782  [INST] zydeco spice mix [/INST] ['paprika', 's...  \n",
       "250783  [INST] zydeco ya ya deviled eggs [/INST] ['har...  \n",
       "250784  [INST] cookies by design   cookies on a stick ...  \n",
       "250785  [INST] cookies by design   sugar shortbread co...  \n",
       "\n",
       "[250786 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert all rows info to llama format \n",
    "df['formatted'] = df.apply(format_rows,axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[INST] Masala Karela Recipe [/INST] 6 Karela (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[INST] Ragi Semiya Upma Recipe - Ragi Millet V...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[INST] Gongura Chicken Curry Recipe - Andhra S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[INST] Pudina Khara Pongal Recipe (Rice and Le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[INST] Udupi Style Ash Gourd Coconut Curry Rec...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  [INST] Masala Karela Recipe [/INST] 6 Karela (...\n",
       "1  [INST] Ragi Semiya Upma Recipe - Ragi Millet V...\n",
       "2  [INST] Gongura Chicken Curry Recipe - Andhra S...\n",
       "3  [INST] Pudina Khara Pongal Recipe (Rice and Le...\n",
       "4  [INST] Udupi Style Ash Gourd Coconut Curry Rec..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = df.rename(columns={'formatted':'text'})\n",
    "new_df = new_df[['text']]\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, eval_df = train_test_split(new_df, test_size=0.26, random_state=42)\n",
    "train_df.to_csv('./datasets/train_inst.csv',index=False)\n",
    "eval_df.to_csv('./datasets/eval_inst.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 185581 examples [00:01, 120426.16 examples/s]\n",
      "Generating train split: 65205 examples [00:00, 128570.60 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 65205\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_ds = load_dataset('csv',data_files='./datasets/train_inst.csv',split='train')\n",
    "training_ds\n",
    "eval_ds = load_dataset('csv',data_files='./datasets/eval_inst.csv',split='train')\n",
    "eval_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning LLaMa Dataset for phi-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  1.62s/it]\n",
      "You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\n",
      "c:\\Python312\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:246: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 185581/185581 [00:18<00:00, 9905.89 examples/s] \n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 65205/65205 [00:07<00:00, 9286.50 examples/s] \n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvishwateja2684\u001b[0m (\u001b[33mvishwa-teja\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\vishw\\Desktop\\chefbot\\model\\wandb\\run-20240607_171546-yqelvgbm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vishwa-teja/huggingface/runs/yqelvgbm' target=\"_blank\">vibrant-thunder-18</a></strong> to <a href='https://wandb.ai/vishwa-teja/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vishwa-teja/huggingface' target=\"_blank\">https://wandb.ai/vishwa-teja/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vishwa-teja/huggingface/runs/yqelvgbm' target=\"_blank\">https://wandb.ai/vishwa-teja/huggingface/runs/yqelvgbm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/2899 [06:16<72:53:44, 90.65s/it] "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 223\u001b[0m\n\u001b[0;32m    212\u001b[0m trainer \u001b[38;5;241m=\u001b[39m SFTTrainer(\n\u001b[0;32m    213\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    214\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtraining_ds,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    219\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args\n\u001b[0;32m    220\u001b[0m )\n\u001b[0;32m    222\u001b[0m \u001b[38;5;66;03m# Train and save the model\u001b[39;00m\n\u001b[1;32m--> 223\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    224\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./Recipe-Generator\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    225\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./Recipe-Generator\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:361\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trl_activate_neftune(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[1;32m--> 361\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[0;32m    364\u001b[0m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\transformers\\trainer.py:1859\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1857\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1858\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1860\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1861\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1864\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\transformers\\trainer.py:2203\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   2202\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 2203\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2206\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2207\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2208\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2209\u001b[0m ):\n\u001b[0;32m   2210\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2211\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\transformers\\trainer.py:3147\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   3145\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m   3146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3147\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\accelerate\\accelerator.py:2011\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2009\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   2010\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2011\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2012\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2013\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available and set the device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# max_sequence_length = 690\n",
    "# def preprocess_example(example):\n",
    "#     example['text'] = example['text'][:max_sequence_length]\n",
    "#     return example\n",
    "# training_ds = training_ds.map(preprocess_example)\n",
    "\n",
    "# model_id = \"microsoft/phi-2\"\n",
    "# new_model = 'mental-health-LLM'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.padding_side = 'right'\n",
    "\n",
    "# try:\n",
    "#     model = AutoModelForCausalLM.from_pretrained(\n",
    "#         model_id,\n",
    "#         trust_remote_code=True,\n",
    "#         low_cpu_mem_usage=True,\n",
    "#         device_map={\"\": 0},\n",
    "#         revision='refs/pr/23'\n",
    "#     )\n",
    "\n",
    "#     model.config.use_cache = False\n",
    "#     model.config.pretraining_tp = 1\n",
    "\n",
    "#     training_args = TrainingArguments(\n",
    "#         output_dir='./mental-healthLLM',\n",
    "#         num_train_epochs=2,\n",
    "#         per_device_train_batch_size=1,  # Reduced batch size for T4\n",
    "#         gradient_accumulation_steps=64,  # Increased to maintain effective batch size\n",
    "#         evaluation_strategy='steps',\n",
    "#         eval_steps=1500,\n",
    "#         optim='paged_adamw_8bit',\n",
    "#         learning_rate=2e-4,\n",
    "#         lr_scheduler_type='cosine',\n",
    "#         save_steps=1500,\n",
    "#         warmup_ratio=0.05,\n",
    "#         weight_decay=0.01,\n",
    "#         max_steps=-1,\n",
    "#         fp16=True  # Enable mixed precision to save memory\n",
    "#     )\n",
    "\n",
    "#     peft_config = LoraConfig(\n",
    "#         r=32,\n",
    "#         lora_alpha=64,\n",
    "#         lora_dropout=0.05,\n",
    "#         bias='none',\n",
    "#         task_type='CAUSAL_LM',\n",
    "#         target_modules=['Wqkv', 'fc1', 'fc2']\n",
    "#     )\n",
    "\n",
    "#     trainer = SFTTrainer(\n",
    "#         model=model,\n",
    "#         train_dataset=training_ds,\n",
    "#         peft_config=peft_config,\n",
    "#         dataset_text_field='text',  # Use the 'formatted' column\n",
    "#         tokenizer=tokenizer,\n",
    "#         args=training_args\n",
    "#     )\n",
    "\n",
    "#     trainer.train()\n",
    "\n",
    "# except Exception as e:\n",
    "#     print('At line:', e.__traceback__.tb_lineno)\n",
    "#     print('________________ERROR________________:', e)\n",
    "\n",
    "# Adjust max sequence length in tokenization\n",
    "# max_sequence_length = 690\n",
    "# def preprocess_example(example):\n",
    "#     example['text'] = example['text'][:max_sequence_length]\n",
    "#     return example\n",
    "# training_ds = training_ds.map(preprocess_example)\n",
    "\n",
    "# model_id = \"microsoft/phi-2\"\n",
    "# new_model = 'mental-health-LLM'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.padding_side = 'right'\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_quant_type='nf4',  # normalizing float 4\n",
    "#     bnb_4bit_compute_dtype=torch.float16,\n",
    "#     bnb_4bit_use_double_quant=False  # Double quantization can degrade the performance\n",
    "# )\n",
    "\n",
    "# try:\n",
    "#     model = AutoModelForCausalLM.from_pretrained(\n",
    "#         model_id,\n",
    "#         trust_remote_code=True,\n",
    "#         # quantization_config=bnb_config,\n",
    "#         low_cpu_mem_usage=True,\n",
    "#         device_map={\"\": 0}\n",
    "#     ).to(device)  # Ensure the model is moved to the correct device\n",
    "\n",
    "#     model.config.use_cache = True  # Enable caching\n",
    "#     model.config.pretraining_tp = 1\n",
    "#     model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n",
    "\n",
    "#     training_args = TrainingArguments(\n",
    "#         output_dir='./mental-healthLLM',\n",
    "#         num_train_epochs=2,\n",
    "#         per_device_train_batch_size=1,  # Reduced batch size for T4 or local GPU with less memory\n",
    "#         gradient_accumulation_steps=64,  # Increased to maintain effective batch size\n",
    "#         evaluation_strategy='steps',\n",
    "#         eval_steps=1500,\n",
    "#         optim='paged_adamw_8bit',\n",
    "#         learning_rate=2e-4,\n",
    "#         lr_scheduler_type='cosine',\n",
    "#         save_steps=1500,\n",
    "#         warmup_ratio=0.05,\n",
    "#         weight_decay=0.01,\n",
    "#         max_steps=-1,\n",
    "#         fp16=True  # Enable mixed precision to save memory\n",
    "#     )\n",
    "\n",
    "#     peft_config = LoraConfig(\n",
    "#         r=32,\n",
    "#         lora_alpha=64,\n",
    "#         lora_dropout=0.05,\n",
    "#         bias='none',\n",
    "#         task_type='CAUSAL_LM',\n",
    "#         target_modules=['Wqkv', 'fc1', 'fc2']\n",
    "#     )\n",
    "\n",
    "#     trainer = SFTTrainer(\n",
    "#         model=model,\n",
    "#         train_dataset=training_ds,\n",
    "#         peft_config=peft_config,\n",
    "#         dataset_text_field='text',  # Use the 'text' column\n",
    "#         tokenizer=tokenizer,\n",
    "#         args=training_args\n",
    "#     )\n",
    "\n",
    "#     trainer.train()\n",
    "\n",
    "# except Exception as e:\n",
    "#     print('At line:', e.__traceback__.tb_lineno)\n",
    "#     print('________________ERROR________________:', e)\n",
    "from transformers import IntervalStrategy\n",
    "\n",
    "model_id = \"microsoft/phi-2\"\n",
    "new_model = 'Recipe-Generator'\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "# BitsAndBytes configuration for 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',  # normalizing float 4\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False  # Avoid double quantization for better performance\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Load model with quantization\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        trust_remote_code=True,\n",
    "        quantization_config=bnb_config,\n",
    "        flash_attn=True,\n",
    "        flash_rotary=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "        device_map={\"\": 0},\n",
    "        revision='refs/pr/23'\n",
    "    )\n",
    "\n",
    "    # Set model configuration for training\n",
    "    model.config.use_cache = False\n",
    "    model.config.pretraining_tp = 1\n",
    "\n",
    "    # Prepare model for k-bit training\n",
    "    model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./Recipe-Generator',\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=1,  # Reduce batch size to fit in memory\n",
    "        gradient_accumulation_steps=64,  # Increase gradient accumulation steps\n",
    "        evaluation_strategy=IntervalStrategy.STEPS,\n",
    "        eval_steps=1500,\n",
    "        optim='paged_adamw_8bit',\n",
    "        learning_rate=2e-4,\n",
    "        lr_scheduler_type='cosine',\n",
    "        save_steps=1500,\n",
    "        warmup_ratio=0.05,\n",
    "        weight_decay=0.01,\n",
    "        fp16=True,  # Use mixed precision\n",
    "        max_steps=-1\n",
    "    )\n",
    "\n",
    "    # PEFT configuration\n",
    "    peft_config = LoraConfig(\n",
    "        r=32,\n",
    "        lora_alpha=64,\n",
    "        lora_dropout=0.05,\n",
    "        bias='none',\n",
    "        task_type='CAUSAL_LM',\n",
    "        target_modules=['Wqkv', 'fc1', 'fc2']\n",
    "    )\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=training_ds,\n",
    "        eval_dataset=eval_ds,\n",
    "        peft_config=peft_config,\n",
    "        dataset_text_field='text',\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_args\n",
    "    )\n",
    "\n",
    "    # Train and save the model\n",
    "    trainer.train()\n",
    "    trainer.save_model('./Recipe-Generator')\n",
    "    tokenizer.save_pretrained('./Recipe-Generator')\n",
    "\n",
    "except Exception as e:\n",
    "    print('At line:', e.__traceback__.tb_lineno)\n",
    "    print('________________ERROR________________:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
